# ensure the results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
library(lattice)
library(ggplot2)
library(e1071)
# load the data
#data(train)
# define the control using a random forest selection function
print('before rfeControl')
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
for (j in 2:2) {
# run the RFE algorithm
print('before rfe')
#results <- rfe(train[,1:10], train[,length(train)], sizes=c(1:10), rfeControl=control)
results <- rfe(train[,regions.start[j]:regions.end[j]], train[,length(train)], sizes=c(1:(regions.end[j]-regions.start[j]+1)), rfeControl=control)
#results <- rfe(train[50:150,1:199], train[50:150,length(train)], sizes=c(1:199), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
#features = results.data.frame
#colnames(features) = cnames
#features$label = train$label
####
}
write.csv(results$optVariables, file="2Reg.csv")
#
#   features = as.data.frame(features)
#   colnames(features) = cnames
#   features$label = train$label
####
} else {
features = train
}
plot(results, type=c("g", "o"))
positive = read.csv('Tyrosine/positive_train5.csv')
negative = read.csv('Tyrosine/negative_train5.csv')
ppm = positive[,1] # First column are the variable names
train = t(cbind(positive[,-1],negative[,-1])) # combine them except the first column
train = as.data.frame(train) # Now convert it to a data frame
train$label = 0 # Now add in a label column initially set to all 0's
train$label[1:94] = 1 # Now set the first 94 samples to 1 to indicate they are positive
train$label = factor(train$label)
colnames(train) = c(ppm,'label') # Now correct the column names
rownames(train) = 1:nrow(train) # Now add the row names
#### This only needs to run once ####
# One way to handle the data is to divide into 5 regions
n.regions = 1
regions.start = c()
regions.end = c()
regions.start[n.regions] = 1
dist.cutoff = 0.01
for (i in 2:length(ppm)) {
if (ppm[i-1]-ppm[i] > dist.cutoff) {
regions.end[n.regions] = i-1
n.regions = n.regions + 1
regions.start[n.regions] = i
}
}
regions.end[n.regions] = ncol(train)-1
####
engineer_features = T
if (engineer_features == T) {
random.order = sample(1:nrow(train),nrow(train))
train = train[random.order,]
# ensure the results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
library(lattice)
library(ggplot2)
library(e1071)
# load the data
#data(train)
# define the control using a random forest selection function
print('before rfeControl')
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
for (j in 5:5) {
# run the RFE algorithm
print('before rfe')
#results <- rfe(train[,1:10], train[,length(train)], sizes=c(1:10), rfeControl=control)
results <- rfe(train[,regions.start[j]:regions.end[j]], train[,length(train)], sizes=c(1:(regions.end[j]-regions.start[j]+1)), rfeControl=control)
#results <- rfe(train[50:150,1:199], train[50:150,length(train)], sizes=c(1:199), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
#features = results.data.frame
#colnames(features) = cnames
#features$label = train$label
####
}
write.csv(results$optVariables, file="5Reg.csv")
#
#   features = as.data.frame(features)
#   colnames(features) = cnames
#   features$label = train$label
####
} else {
features = train
}
plot(results, type=c("g", "o"))
pdf(file=paste(j,"RegORG.csv",sep=""), width= 3.5, height = 3.5)
plot(results, type=c("g", "o"))
dev.off()
pdf(file=paste(j,"RegORG.pdf",sep=""), width= 3.5, height = 3.5)
plot(results, type=c("g", "o"))
dev.off()
pdf(file=paste(j,"RegORG.pdf",sep=""), width= 7, height = 3.5)
plot(results, type=c("g", "o"))
dev.off()
pdf(file=paste(j,"RegORG.pdf",sep=""), width= 7, height = 3.5)
plot(results, type=c("g", "o"))
dev.off()
j=2
pdf(file=paste(j,"RegORG.pdf",sep=""), width= 7, height = 3.5)
plot(results, type=c("g", "o"))
dev.off()
j=3
pdf(file=paste(j,"RegORG.pdf",sep=""), width= 7, height = 3.5)
plot(results, type=c("g", "o"))
dev.off()
j=3
pdf(file=paste("ORG_",j,".pdf",sep=""), width= 7, height = 3.5)
plot(results, type=c("g", "o"))
dev.off()
write.csv(results$optVariables, paste("ORGset_",j,".pdf",sep=""))
write.csv(results$optVariables, paste("ORGset_",j,".csv",sep=""))
write.csv(results$optVariables, file=paste("ORGset_",j,".csv",sep=""))
positive = read.csv('Tyrosine/positive_train.csv')
negative = read.csv('Tyrosine/negative_train.csv')
ppm = positive[,1] # First column are the variable names
train = t(cbind(positive[,-1],negative[,-1])) # combine them except the first column
train = as.data.frame(train) # Now convert it to a data frame
train$label = 0 # Now add in a label column initially set to all 0's
train$label[1:94] = 1 # Now set the first 94 samples to 1 to indicate they are positive
train$label = factor(train$label)
colnames(train) = c(ppm,'label') # Now correct the column names
rownames(train) = 1:nrow(train) # Now add the row names
#### This only needs to run once ####
# One way to handle the data is to divide into 5 regions
n.regions = 1
regions.start = c()
regions.end = c()
regions.start[n.regions] = 1
dist.cutoff = 0.01
for (i in 2:length(ppm)) {
if (ppm[i-1]-ppm[i] > dist.cutoff) {
regions.end[n.regions] = i-1
n.regions = n.regions + 1
regions.start[n.regions] = i
}
}
regions.end[n.regions] = ncol(train)-1
####
engineer_features = T
if (engineer_features == T) {
# ensure the results are repeatable
set.seed(7)
random.order = sample(1:nrow(train),nrow(train))
train = train[random.order,]
# load the library
library(mlbench)
library(caret)
library(lattice)
library(ggplot2)
library(e1071)
# load the data
#data(train)
# define the control using a random forest selection function
print('before rfeControl')
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
for (j in 1:n.regions) {
# run the RFE algorithm
print('before rfe')
#results <- rfe(train[,1:10], train[,length(train)], sizes=c(1:10), rfeControl=control)
results <- rfe(train[,regions.start[j]:regions.end[j]], train[,length(train)], sizes=c(1:(regions.end[j]-regions.start[j]+1)), rfeControl=control)
#results <- rfe(train[50:150,1:199], train[50:150,length(train)], sizes=c(1:199), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
pdf(file=paste("ORGplot_",j,".pdf",sep=""), width= 7, height = 3.5)
plot(results, type=c("g", "o"))
dev.off()
#features = results.data.frame
#colnames(features) = cnames
#features$label = train$label
####
write.csv(results$optVariables, file=paste("ORGset_",j,".csv",sep=""))
}
#
#   features = as.data.frame(features)
#   colnames(features) = cnames
#   features$label = train$label
####
} else {
features = train
}
positive = read.csv('Tyrosine/positive_train.csv')
negative = read.csv('Tyrosine/negative_train.csv')
ppm = positive[,1] # First column are the variable names
train = t(cbind(positive[,-1],negative[,-1])) # combine them except the first column
train = as.data.frame(train) # Now convert it to a data frame
train$label = 0 # Now add in a label column initially set to all 0's
train$label[1:94] = 1 # Now set the first 94 samples to 1 to indicate they are positive
train$label = factor(train$label)
colnames(train) = c(ppm,'label') # Now correct the column names
rownames(train) = 1:nrow(train) # Now add the row names
#### This only needs to run once ####
# One way to handle the data is to divide into 5 regions
n.regions = 1
regions.start = c()
regions.end = c()
regions.start[n.regions] = 1
dist.cutoff = 0.01
for (i in 2:length(ppm)) {
if (ppm[i-1]-ppm[i] > dist.cutoff) {
regions.end[n.regions] = i-1
n.regions = n.regions + 1
regions.start[n.regions] = i
}
}
regions.end[n.regions] = ncol(train)-1
####
engineer_features = T
if (engineer_features == T) {
# ensure the results are repeatable
set.seed(7)
random.order = sample(1:nrow(train),nrow(train))
train = train[random.order,]
# load the library
library(mlbench)
library(caret)
library(lattice)
library(ggplot2)
library(e1071)
# load the data
#data(train)
# define the control using a random forest selection function
print('before rfeControl')
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
for (j in 1:n.regions) {
# run the RFE algorithm
print('before rfe')
#results <- rfe(train[,1:10], train[,length(train)], sizes=c(1:10), rfeControl=control)
results <- rfe(train[,regions.start[j]:regions.end[j]], train[,length(train)], sizes=c(1:(regions.end[j]-regions.start[j]+1)), rfeControl=control)
#results <- rfe(train[50:150,1:199], train[50:150,length(train)], sizes=c(1:199), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
pdf(file=paste("ORGplot_",j,".pdf",sep=""), width= 7, height = 3.5)
plot(results, type=c("g", "o"))
dev.off()
#features = results.data.frame
#colnames(features) = cnames
#features$label = train$label
####
write.csv(results$optVariables, file=paste("ORGset_",j,".csv",sep=""))
}
#
#   features = as.data.frame(features)
#   colnames(features) = cnames
#   features$label = train$label
####
} else {
features = train
}
library(mlbench)
library(caret)
library(lattice)
library(ggplot2)
library(e1071)
positive = read.csv('Tyrosine/positive_train.csv')
negative = read.csv('Tyrosine/negative_train.csv')
ppm = positive[,1] # First column are the variable names
train = t(cbind(positive[,-1],negative[,-1])) # combine them except the first column
train = as.data.frame(train) # Now convert it to a data frame
train$label = 0 # Now add in a label column initially set to all 0's
train$label[1:94] = 1 # Now set the first 94 samples to 1 to indicate they are positive
train$label = factor(train$label)
colnames(train) = c(ppm,'label') # Now correct the column names
rownames(train) = 1:nrow(train) # Now add the row names
#### This only needs to run once ####
# One way to handle the data is to divide into 5 regions
n.regions = 1
regions.start = c()
regions.end = c()
regions.start[n.regions] = 1
dist.cutoff = 0.01
for (i in 2:length(ppm)) {
if (ppm[i-1]-ppm[i] > dist.cutoff) {
regions.end[n.regions] = i-1
n.regions = n.regions + 1
regions.start[n.regions] = i
}
}
regions.end[n.regions] = ncol(train)-1
####
library(mlbench)
library(caret)
library(lattice)
library(mlbench)
library(caret)
library(lattice)
library(ggplot2)
library(e1071)
library(mlbench)
library(caret)
library(mlbench)
library(lattice)
library(ggplot2)
library(caret)
library(e1071)
positive = read.csv('Tyrosine/positive_train.csv')
negative = read.csv('Tyrosine/negative_train.csv')
ppm = positive[,1] # First column are the variable names
train = t(cbind(positive[,-1],negative[,-1])) # combine them except the first column
train = as.data.frame(train) # Now convert it to a data frame
train$label = 0 # Now add in a label column initially set to all 0's
train$label[1:94] = 1 # Now set the first 94 samples to 1 to indicate they are positive
train$label = factor(train$label)
rownames(train) = 1:nrow(train) # Now add the row names
# One way to handle the data is to divide into 5 regions
regions.start = c()
regions.start[n.regions] = 1
dist.cutoff = 0.01
if (ppm[i-1]-ppm[i] > dist.cutoff) {
n.regions = n.regions + 1
}
n.regions = 1
colnames(train) = c(ppm,'label') # Now correct the column names
for (i in 2:length(ppm)) {
regions.end[n.regions] = ncol(train)-1
regions.end = c()
regions.end[n.regions] = i-1
#### This only needs to run once ####
regions.start[n.regions] = i
}
####
library(mlbench)
library(lattice)
library(ggplot2)
library(caret)
library(e1071)
positive = read.csv('Tyrosine/positive_train.csv')
negative = read.csv('Tyrosine/negative_train.csv')
ppm = positive[,1] # First column are the variable names
train = t(cbind(positive[,-1],negative[,-1])) # combine them except the first column
train = as.data.frame(train) # Now convert it to a data frame
train$label = 0 # Now add in a label column initially set to all 0's
train$label[1:94] = 1 # Now set the first 94 samples to 1 to indicate they are positive
train$label = factor(train$label)
colnames(train) = c(ppm,'label') # Now correct the column names
rownames(train) = 1:nrow(train) # Now add the row names
#### This only needs to run once ####
# One way to handle the data is to divide into 5 regions
n.regions = 1
regions.start = c()
regions.end = c()
regions.start[n.regions] = 1
dist.cutoff = 0.01
for (i in 2:length(ppm)) {
if (ppm[i-1]-ppm[i] > dist.cutoff) {
regions.end[n.regions] = i-1
n.regions = n.regions + 1
regions.start[n.regions] = i
}
}
regions.end[n.regions] = ncol(train)-1
####
engineer_features = T
if (engineer_features == T) {
# ensure the results are repeatable
set.seed(7)
random.order = sample(1:nrow(train),nrow(train))
train = train[random.order,]
# load the library
library(mlbench)
library(lattice)
library(ggplot2)
library(caret)
library(e1071)
# load the data
#data(train)
# define the control using a random forest selection function
print('before rfeControl')
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
for (j in 1:n.regions) {
# run the RFE algorithm
print('before rfe')
#results <- rfe(train[,1:10], train[,length(train)], sizes=c(1:10), rfeControl=control)
results <- rfe(train[,regions.start[j]:regions.end[j]], train[,length(train)], sizes=c(1:(regions.end[j]-regions.start[j]+1)), rfeControl=control)
#results <- rfe(train[50:150,1:199], train[50:150,length(train)], sizes=c(1:199), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
pdf(file=paste("ORGplot_",j,".pdf",sep=""), width= 7, height = 3.5)
plot(results, type=c("g", "o"))
dev.off()
#features = results.data.frame
#colnames(features) = cnames
#features$label = train$label
####
write.csv(results$optVariables, file=paste("ORGset_",j,".csv",sep=""))
}
#
#   features = as.data.frame(features)
#   colnames(features) = cnames
#   features$label = train$label
####
} else {
features = train
}
positive = read.csv('Tyrosine/positive_train.csv')
negative = read.csv('Tyrosine/negative_train.csv')
ppm = positive[,1] # First column are the variable names
train = t(cbind(positive[,-1],negative[,-1])) # combine them except the first column
train = as.data.frame(train) # Now convert it to a data frame
train$label = 0 # Now add in a label column initially set to all 0's
train$label[1:94] = 1 # Now set the first 94 samples to 1 to indicate they are positive
train$label = factor(train$label)
colnames(train) = c(ppm,'label') # Now correct the column names
rownames(train) = 1:nrow(train) # Now add the row names
#### This only needs to run once ####
# One way to handle the data is to divide into 5 regions
n.regions = 1
regions.start = c()
regions.end = c()
regions.start[n.regions] = 1
dist.cutoff = 0.01
for (i in 2:length(ppm)) {
if (ppm[i-1]-ppm[i] > dist.cutoff) {
regions.end[n.regions] = i-1
n.regions = n.regions + 1
regions.start[n.regions] = i
}
}
regions.end[n.regions] = ncol(train)-1
####
engineer_features = T
if (engineer_features == T) {
# ensure the results are repeatable
set.seed(7)
random.order = sample(1:nrow(train),nrow(train))
train = train[random.order,]
# load the library
library(mlbench)
library(lattice)
library(ggplot2)
library(caret)
library(e1071)
# load the data
#data(train)
# define the control using a random forest selection function
print('before rfeControl')
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
for (j in 1:n.regions) {
# run the RFE algorithm
print('before rfe')
#results <- rfe(train[,1:10], train[,length(train)], sizes=c(1:10), rfeControl=control)
results <- rfe(train[,regions.start[j]:regions.end[j]], train[,length(train)], sizes=c(1:(regions.end[j]-regions.start[j]+1)), rfeControl=control)
#results <- rfe(train[50:150,1:199], train[50:150,length(train)], sizes=c(1:199), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
pdf(file=paste("ORGplot_",j,".pdf",sep=""), width= 7, height = 3.5)
plot(results, type=c("g", "o"))
dev.off()
#features = results.data.frame
#colnames(features) = cnames
#features$label = train$label
####
write.csv(results$optVariables, file=paste("ORGset_",j,".csv",sep=""))
}
#
#   features = as.data.frame(features)
#   colnames(features) = cnames
#   features$label = train$label
####
} else {
features = train
}
pdf(file=paste("ORGplot_",j,".pdf",sep=""), width= 7, height = 3.5)
plot(results, type=c("g", "o"))
dev.off()
load("~/Dropbox/GitHub/Metabolite-Prediction-Group/.RData")
con <- file("test.log")
sink(con, append=TRUE)
sink(con, append=TRUE, type="message")
# This will echo all input and not truncate 150+ character lines...
source("script.R", echo=TRUE, max.deparse.length=10000)
# Restore output to console
sink()
sink(type="message")
# And look at the log...
cat(readLines("test.log"), sep="\n")
